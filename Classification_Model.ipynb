{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "This exercise uses a binary classifcation data set called Food_Classifier.csv to predict labels (food and non-food).\n",
    "Sinse the data set is labelled, supervised learning data mining is used to infer labeled training data. \n",
    "\n",
    "A text classification is performed using tf-idf (term frequency-inverse document frequency) approach that weighs the importance of features in a document and also in a collection of documents.\n",
    "\n",
    "Finally random forest classifier is used to get predictions. \n",
    "Alternatively Support Vector Machines (SVM) was also explored to check the consistency of the two predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "#nltk.download('popular') # Use this to download all popular nltk data\n",
    "import sklearn\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import svm\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The functions below are created to input new test data. These are used to ouput model performance metrics such as accuracy and confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def food_classifier(filename, vectorizer):\n",
    "    # Read data\n",
    "    raw_expense = pd.read_csv(filename,encoding = \"ISO-8859-1\")\n",
    "    \n",
    "    #Replace Nan with blanks\n",
    "    raw_expense = raw_expense.replace(np.nan, '', regex=True)\n",
    "    \n",
    "    #Join columns\n",
    "    raw_expense['expense_text'] = raw_expense['type'] +' '+ raw_expense['description']\n",
    "    \n",
    "    #Extract columns\n",
    "    cleaned_expense = raw_expense[['expense_text', 'is_food']]\n",
    "\n",
    "    # Replace white space with NA\n",
    "    # Drop NA values\n",
    "    cleaned_expense = cleaned_expense.replace(r'', np.nan, regex=True).dropna().reset_index(drop = True)\n",
    "    \n",
    "    # Change text to lower case\n",
    "    cleaned_expense['expense_text'] = cleaned_expense['expense_text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    \n",
    "    # Replace special charaters with white space\n",
    "    cleaned_expense['expense_text'] = cleaned_expense['expense_text'].str.replace('[^\\w\\s]',' ')\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    cleaned_expense['expense_text']  = cleaned_expense['expense_text'] .apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n",
    "\n",
    "    # Lemmatize text\n",
    "    cleaned_expense['expense_text'] = cleaned_expense['expense_text'].apply(lambda row: nltk.stem.WordNetLemmatizer().lemmatize(row))\n",
    "\n",
    "    # Split data to train and split\n",
    "    # Data is impalanced- Use stratified sampling\n",
    "\n",
    "    X,Y = cleaned_expense.drop('is_food',axis=1), cleaned_expense['is_food']\n",
    "    Y = Y.apply(lambda x: 1 if x == 'food' else 0)\n",
    "    X = X.values\n",
    "    X = pd.DataFrame(X)\n",
    "    \n",
    "    # Convert train and test text data to pandas data frame\n",
    "    # NOTE: TfidfVectorizer is used to \n",
    "    # convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "    # output is a term-document matrix\n",
    "\n",
    "    X_tfidf = vectorizer.transform(X[0])\n",
    "    \n",
    "    return(X_tfidf, Y.values)\n",
    "    \n",
    "\n",
    "def rf_classifier(classifier, X_Test, Y_Test):\n",
    "    \n",
    "    pred = classifier.predict(X_Test)\n",
    "    \n",
    "    acc = accuracy_score(Y_Test, pred)\n",
    "    \n",
    "    conf = confusion_matrix(Y_Test, pred)\n",
    "    \n",
    "    return acc,conf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration and building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>country</th>\n",
       "      <th>amount</th>\n",
       "      <th>amount_per_day</th>\n",
       "      <th>claim_type</th>\n",
       "      <th>is_food</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Lunch when  at NON client site</td>\n",
       "      <td>Lunch at BBC Millbank, non-client site</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>4.8230</td>\n",
       "      <td>4.8230</td>\n",
       "      <td>Receipt</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Parking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>2.6558</td>\n",
       "      <td>2.6558</td>\n",
       "      <td>Receipt</td>\n",
       "      <td>non-food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Parking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Italy</td>\n",
       "      <td>1.1999</td>\n",
       "      <td>1.1999</td>\n",
       "      <td>Receipt</td>\n",
       "      <td>non-food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Other Expenses-  Taxi</td>\n",
       "      <td>TAKSI</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>15.9489</td>\n",
       "      <td>1.1392</td>\n",
       "      <td>Receipt</td>\n",
       "      <td>non-food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Onsite/offsite support - FB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>China</td>\n",
       "      <td>13.4651</td>\n",
       "      <td>13.4651</td>\n",
       "      <td>Receipt</td>\n",
       "      <td>non-food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                            type  \\\n",
       "0  0           Lunch when  at NON client site   \n",
       "1  1           Parking                          \n",
       "2  2           Parking                          \n",
       "3  3           Other Expenses-  Taxi            \n",
       "4  4           Onsite/offsite support - FB      \n",
       "\n",
       "                              description         country   amount  \\\n",
       "0  Lunch at BBC Millbank, non-client site  United Kingdom  4.8230    \n",
       "1  NaN                                     Brazil          2.6558    \n",
       "2  NaN                                     Italy           1.1999    \n",
       "3  TAKSI                                   Turkey          15.9489   \n",
       "4  NaN                                     China           13.4651   \n",
       "\n",
       "   amount_per_day claim_type   is_food  \n",
       "0  4.8230          Receipt    food      \n",
       "1  2.6558          Receipt    non-food  \n",
       "2  1.1999          Receipt    non-food  \n",
       "3  1.1392          Receipt    non-food  \n",
       "4  13.4651         Receipt    non-food  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file\n",
    "# Set encoding for text: Text contains numbers, upper and lowercase English letters\n",
    "raw_expense = pd.read_csv('Food_Classifier.csv',encoding = \"ISO-8859-1\")\n",
    "raw_expense.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">amount_per_day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_food</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>0.0109</td>\n",
       "      <td>1150.5644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non-food</th>\n",
       "      <td>0.0032</td>\n",
       "      <td>10008.1581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         amount_per_day            \n",
       "                    min         max\n",
       "is_food                            \n",
       "food      0.0109         1150.5644 \n",
       "non-food  0.0032         10008.1581"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make copy of raw data\n",
    "copy_expense = raw_expense.copy()\n",
    "\n",
    "# Get absolute value of amount data\n",
    "copy_expense['amount_per_day'] = abs(copy_expense['amount_per_day'])\n",
    "\n",
    "# Get amount value greater than 0\n",
    "copy_expense['amount_per_day'] = copy_expense['amount_per_day'][copy_expense['amount_per_day'] > 0]\n",
    "\n",
    "# Get min and max values\n",
    "copy_expense.groupby('is_food').agg({'amount_per_day': [min, max]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Receipt', 'Per diem'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy_expense['claim_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace Nan with blanks\n",
    "raw_expense = raw_expense.replace(np.nan, '', regex=True)\n",
    "\n",
    "# Join columns\n",
    "raw_expense['expense_text'] = raw_expense['type'] +' '+ raw_expense['description']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56211"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract columns\n",
    "cleaned_expense = raw_expense[['expense_text', 'is_food']]\n",
    "\n",
    "# Number of observations\n",
    "cleaned_expense.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55141"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace white space with NA\n",
    "# Drop NA values\n",
    "\n",
    "cleaned_expense = cleaned_expense.replace(r'', np.nan, regex=True).dropna().reset_index(drop = True)\n",
    "\n",
    "# Number of observations\n",
    "cleaned_expense.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing textual data\n",
    "\n",
    "# Change to lower case\n",
    "# Splits text to different characters, converts to lower and joins them back\n",
    "\n",
    "cleaned_expense['expense_text'] = cleaned_expense['expense_text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expense_text</th>\n",
       "      <th>is_food</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lunch when at non client site lunch at bbc millbank  non client site</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>parking</td>\n",
       "      <td>non-food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>parking</td>\n",
       "      <td>non-food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other expenses  taxi taksi</td>\n",
       "      <td>non-food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>onsite offsite support   fb</td>\n",
       "      <td>non-food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           expense_text  \\\n",
       "0  lunch when at non client site lunch at bbc millbank  non client site   \n",
       "1  parking                                                                \n",
       "2  parking                                                                \n",
       "3  other expenses  taxi taksi                                             \n",
       "4  onsite offsite support   fb                                            \n",
       "\n",
       "    is_food  \n",
       "0  food      \n",
       "1  non-food  \n",
       "2  non-food  \n",
       "3  non-food  \n",
       "4  non-food  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace special characters with white space\n",
    "\n",
    "cleaned_expense['expense_text'] = cleaned_expense['expense_text'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "# Check first 5 rows\n",
    "cleaned_expense.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "# Splits text to different characters, remove stop words and joins them back\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "cleaned_expense['expense_text']  = cleaned_expense['expense_text'] .apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lemmatize text, arrives at a common root form of the word\n",
    "\n",
    "cleaned_expense['expense_text'] = cleaned_expense['expense_text'].apply(lambda x: \" \".join(nltk.stem.WordNetLemmatizer().lemmatize(x) for x in x.split()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expense_text</th>\n",
       "      <th>is_food</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lunch non client site lunch bbc millbank non client site</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>parking</td>\n",
       "      <td>non-food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>parking</td>\n",
       "      <td>non-food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>expense taxi taksi</td>\n",
       "      <td>non-food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>onsite offsite support fb</td>\n",
       "      <td>non-food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               expense_text   is_food\n",
       "0  lunch non client site lunch bbc millbank non client site  food    \n",
       "1  parking                                                   non-food\n",
       "2  parking                                                   non-food\n",
       "3  expense taxi taksi                                        non-food\n",
       "4  onsite offsite support fb                                 non-food"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check first 5 rows\n",
    "cleaned_expense.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_food\n",
       "food        0.28059\n",
       "non-food    0.71941\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check porportions of is_food\n",
    "\n",
    "cleaned_expense.groupby(['is_food']).size()/len(cleaned_expense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data to train and split\n",
    "# Data is impalanced- Use stratified sampling\n",
    "\n",
    "# Extract text and label data\n",
    "# Assign to X and Y\n",
    "X,Y = cleaned_expense.drop('is_food',axis=1), cleaned_expense['is_food']\n",
    "\n",
    "# Classify labels as 0 and 1\n",
    "# In this, food is 1 and non food is 0\n",
    "Y = Y.apply(lambda x: 1 if x == 'food' else 0)\n",
    "\n",
    "#Convert to arrays\n",
    "X,Y = X.values,Y.values\n",
    "\n",
    "# X : feature text\n",
    "# Y : labels\n",
    "\n",
    "#70-30 data split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y,\n",
    "                                                    test_size=0.30,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify=Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38598"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of observations\n",
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16543"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of observations\n",
    "X_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38598, 10164)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert train and test text data to pandas data frame\n",
    "X_train = pd.DataFrame(X_train)\n",
    "\n",
    "X_val = pd.DataFrame(X_val)\n",
    "\n",
    "# Use tfidf vectorizer for feature extraction\n",
    "# NOTE: TfidfVectorizer is used to \n",
    "# convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "# Output is a term-document matrix\n",
    "\n",
    "# Call tfidf vectorizer function\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "#apply tfids vectorizer on train data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train[0])\n",
    "\n",
    "# Check matrix dimensions\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16543, 10164)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use fitted vectorizer on the testing data\n",
    "X_val_tfidf = vectorizer.transform(X_val[0])\n",
    "\n",
    "# Check matrix dimensions\n",
    "X_val_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.7763%\n",
      "True Negatives: 11882\n",
      "False Postives: 19\n",
      "False Negatives: 18\n",
      "True Postives: 4624\n",
      "Precision: 99.5908%\n",
      "Recall: 99.6122%\n"
     ]
    }
   ],
   "source": [
    "# Random forest classifier model\n",
    "# No of estimators is no of trees the forest should have\n",
    "# Minimum sample split in miminum number of sample maintained in each node\n",
    "classifier = RandomForestClassifier(n_estimators = 500, min_samples_split=100, criterion = \"gini\", random_state = 42)\n",
    "\n",
    "# Fit classifier to training set\n",
    "classifier = classifier.fit(X_train_tfidf, Y_train)\n",
    "\n",
    "# Predict on test data using the fitted classifier\n",
    "pred = classifier.predict(X_val_tfidf)\n",
    "\n",
    "# Check for accuracy between the actual and precited labels\n",
    "acc = accuracy_score(Y_val, pred)\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf = confusion_matrix(Y_val, pred)\n",
    "\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "\n",
    "# Estimate precision and recall\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "\n",
    "print('Accuracy: {}%'.format(round(acc*100,4)))\n",
    "print('True Negatives: {}'.format(tn))\n",
    "print('False Postives: {}'.format(fp))\n",
    "print('False Negatives: {}'.format(fn))\n",
    "print('True Postives: {}'.format(tp))\n",
    "print('Precision: {}%'.format(round(precision*100,4)))\n",
    "print('Recall: {}%'.format(round(recall*100,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.6373%\n",
      "True Negatives: 11872\n",
      "False Postives: 29\n",
      "False Negatives: 31\n",
      "True Postives: 4611\n",
      "Precision: 99.375%\n",
      "Recall: 99.3322%\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine classifier\n",
    "classifier = svm.SVC(gamma=0.01, C=100., random_state = 42)\n",
    "\n",
    "# Fit classifier to training set\n",
    "classifier = classifier.fit(X_train_tfidf, Y_train) \n",
    "\n",
    "# Predict on test data using the fitted classifier\n",
    "pred = classifier.predict(X_val_tfidf)\n",
    "\n",
    "# Check for accuracy between the actual and precited labels\n",
    "acc = accuracy_score(Y_val, pred)\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf = confusion_matrix(Y_val, pred)\n",
    "\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "\n",
    "# Estimate precision and recall\n",
    "precision = tp/(tp+fp)\n",
    "\n",
    "recall = tp/(tp+fn)\n",
    "\n",
    "print('Accuracy: {}%'.format(round(acc*100,4)))\n",
    "print('True Negatives: {}'.format(tn))\n",
    "print('False Postives: {}'.format(fp))\n",
    "print('False Negatives: {}'.format(fn))\n",
    "print('True Postives: {}'.format(tp))\n",
    "print('Precision: {}%'.format(round(precision*100,4)))\n",
    "print('Recall: {}%'.format(round(recall*100,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Testing New Data\n",
    "\n",
    "#### The functions created earlier in the file are called here to print model performance metrics for the new test data.\n",
    "#### The functions defined earlier must be run to avoid errors below.\n",
    "##### ***Note : These will be left uncommented until new data is input.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Input new test data\n",
    "# filename='Food_Classifier.csv'\n",
    "\n",
    "# #Get the new tfidf and is_food labels from new data\n",
    "# X_tfidf_new, Y_new = food_classifier(filename, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Call the new tfidf matrix and is food labels here\n",
    "# X_Test,Y_Test = X_tfidf_new, Y_new\n",
    "\n",
    "# # Run Randomforest model on the new data\n",
    "# accuracy, conf_matrix = rf_classifier(classifier, X_Test, Y_Test)\n",
    "# tn, fp, fn, tp = conf_matrix.ravel()\n",
    "# precision = tp/(tp+fp)\n",
    "# recall = tp/(tp+fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('Accuracy: {}%'.format(round(acc*100,4)))\n",
    "# print('True Negatives: {}'.format(tn))\n",
    "# print('False Postives: {}'.format(fp))\n",
    "# print('False Negatives: {}'.format(fn))\n",
    "# print('True Postives: {}'.format(tp))\n",
    "# print('Precision: {}%'.format(round(precision*100,4)))\n",
    "# print('Recall: {}%'.format(round(recall*100,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### References\n",
    "- http://scikit-learn.org/\n",
    "- https://towardsdatascience.com/\n",
    "- https://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria\n",
    "- http://scikit-learn.org/stable/supervised_learning.html\n",
    "- https://stackoverflow.com/questions/18171739/unicodedecodeerror-when-reading-csv-file-in-pandas-with-python\n",
    "- http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex8/ex8.html\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
